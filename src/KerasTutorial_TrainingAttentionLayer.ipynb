{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KerasTutorial-TrainingAttentionLayer.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jorcisai/ARF/blob/master/src/KerasTutorial_TrainingAttentionLayer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWeQAo0Ec_BL"
      },
      "source": [
        "#Bilingual text classifier using own trained word embeddings and Attention layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgZ9gjmPfSnK"
      },
      "source": [
        "## Imports\n",
        "Importing standard packages and tensorflow_datasets to ease data manipulation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baYFZMW_bJHh"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "  \n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DtIDiZkhAaB"
      },
      "source": [
        "Data loading from local file system. Please use the \"traveler\" dataset available in the \"dat\" directory of the GitHub:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uS3QuPcgm92"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  datafn=fn;\n",
        "\n",
        "print(\"Data file: \",datafn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vycakl4thjN2"
      },
      "source": [
        "## Load text data from local file\n",
        "\n",
        "Parsing file line by line to extract class label, source and target sentences. All three are lists of strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8L-vKS7Bh-CJ"
      },
      "source": [
        "numsamples=0\n",
        "labs=[]\n",
        "srcs=[]\n",
        "trgs=[]\n",
        "for line in open(datafn):\n",
        "  numsamples+=1\n",
        "  words = line.split(\" \")\n",
        "  labs.append(words[0])\n",
        "  pos=words.index(\"#\")\n",
        "  srcs.append(\" \".join(words[1:pos-1]))\n",
        "  trgs.append(\" \".join(words[pos+1:]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtZVTyI3nQ5i"
      },
      "source": [
        "Loading class labels, source (Spanish) and target (English) sentences from lists into dataset objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_CsoAaPlz8l"
      },
      "source": [
        "labs_dataset = tf.data.Dataset.from_tensor_slices(labs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0Vu4Ge2pwZa"
      },
      "source": [
        "srcs_dataset = tf.data.Dataset.from_tensor_slices(srcs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yLSKCB0F_Jr"
      },
      "source": [
        "trgs_dataset = tf.data.Dataset.from_tensor_slices(trgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJl1pEUPHoxe"
      },
      "source": [
        "Taking a look at the class labels and source sentences after being converted into dataset type. In this example, the source sentences are used to train the model including word embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPsynLIHICqC"
      },
      "source": [
        "for lab in labs_dataset.take(5):\n",
        "  print(lab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_eCQoF3p70Z"
      },
      "source": [
        "for src in srcs_dataset.take(5):\n",
        "  print(src)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87aa69dURsRj"
      },
      "source": [
        "for trg in trgs_dataset.take(5):\n",
        "  print(trg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h2L5p67OcOF"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk6hgqNgOjG2"
      },
      "source": [
        "Obtaining the set of class labels to map them into integers and computing the number of classes. It requires to extract the string from the tf.Tensor object and then map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iUCjqqNOk5q"
      },
      "source": [
        "label_set = set()\n",
        "for lab_tensor in labs_dataset:\n",
        "  label_set.add(lab_tensor.numpy().decode('utf-8'))\n",
        "  \n",
        "num_classes=len(label_set)\n",
        "\n",
        "lab2id = {}\n",
        "for lab_id,lab in enumerate(label_set):\n",
        "  lab2id[lab]=lab_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHVpmcjIOpmv"
      },
      "source": [
        "You can check the assignment of class label to integer label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzWj8WjSOtAM"
      },
      "source": [
        "for key,value in lab2id.items():\n",
        "  print (key,value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEgTAK3M_JQJ"
      },
      "source": [
        "Apply `Dataset.map` to each element of the dataset using the encoder as a function. `Dataset.map` runs in graph mode.\n",
        "\n",
        "* Graph tensors do not have a value. \n",
        "* In graph mode you can only use TensorFlow Ops and functions. \n",
        "\n",
        "So you can't `.map` this function directly: You need to wrap it in a `tf.py_function`. The `tf.py_function` will pass regular tensors (with a value and a `.numpy()` method to access it), to the wrapped python function.\n",
        "\n",
        "Converting author labels <b>A</b>, <b>F</b>, <b>J</b> and <b>P</b> into integers for the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IyQM-54ne08"
      },
      "source": [
        "def author_labeler(text_lab: tf.Tensor):\n",
        "  int_lab = lab2id[text_lab.numpy().decode('utf-8')]\n",
        "  return int_lab\n",
        "\n",
        "def lab_map_fn(text_lab):\n",
        "  # py_func doesn't set the shape of the returned tensors.\n",
        "  int_lab = tf.py_function(func=author_labeler, inp=[text_lab], Tout=tf.int64)\n",
        "  # tf.data.Datasets need to set the shapes manually\n",
        "  int_lab.set_shape([])\n",
        "  return int_lab\n",
        "\n",
        "labs_encoded_dataset = labs_dataset.map(lab_map_fn)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W_hx64NHWsN"
      },
      "source": [
        "You can check how a few class samples look like after being mapped "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02-jGUtioX0-"
      },
      "source": [
        "for lab in labs_encoded_dataset.take(5):\n",
        "  print(lab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPA5d3kcoHVr"
      },
      "source": [
        "Build the vocabulary of a set of sentences by tokenizing each sentence and adding the resulting tokens into a set of of individual unique words. For this tutorial:\n",
        "<ol>\n",
        "<li> Iterate over each sentence's numpy value.</li>\n",
        "<li> Use tfds.deprecated.text.Tokenizer to split it into tokens.</li>\n",
        "<li> Collect these tokens into a Python set, to remove duplicates.</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOt34nTOAm48"
      },
      "source": [
        "tokenizer = tfds.deprecated.text.Tokenizer()\n",
        "\n",
        "src_vocabulary_set = set()\n",
        "for text_tensor in srcs_dataset:\n",
        "  tokens = tokenizer.tokenize(text_tensor.numpy())\n",
        "  src_vocabulary_set.update(tokens)\n",
        "  \n",
        "src_vocab_size=len(src_vocabulary_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85b8QjXSGkV1"
      },
      "source": [
        "print(src_vocabulary_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nAfYR6akwkD"
      },
      "source": [
        "print(src_vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnHw8uXsPGlx"
      },
      "source": [
        "tokenizer = tfds.deprecated.text.Tokenizer()\n",
        "\n",
        "trg_vocabulary_set = set()\n",
        "for text_tensor in trgs_dataset:\n",
        "  tokens = tokenizer.tokenize(text_tensor.numpy())\n",
        "  trg_vocabulary_set.update(tokens)\n",
        "  \n",
        "trg_vocab_size=len(trg_vocabulary_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFN_sM4EPO9I"
      },
      "source": [
        "print(trg_vocabulary_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avwcaNj7POv4"
      },
      "source": [
        "print(trg_vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylqvz4D4pNhG"
      },
      "source": [
        "Create an encoder by passing the vocabulary_set to tfds.deprecated.text.TokenTextEncoder. The encoder's encode method takes in a string of text and returns a list of integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ama-wKuyBLkZ"
      },
      "source": [
        "src_encoder = tfds.deprecated.text.TokenTextEncoder(src_vocabulary_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCPJjlXxPaSQ"
      },
      "source": [
        "trg_encoder = tfds.deprecated.text.TokenTextEncoder(trg_vocabulary_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVMcUXwcEoXn"
      },
      "source": [
        "for src in srcs_dataset.take(5):\n",
        "  print(src_encoder.encode(src.numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gK9EUxJNPf6r"
      },
      "source": [
        "for trg in trgs_dataset.take(5):\n",
        "  print(trg_encoder.encode(trg.numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kplPAlVwpc3v"
      },
      "source": [
        "Mapping tokens of source sentences into list of integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTZ4qc3bCfQ_"
      },
      "source": [
        "def src_encode(text_tensor):\n",
        "  src_encoded_text = src_encoder.encode(text_tensor.numpy())\n",
        "  return [src_encoded_text]\n",
        "\n",
        "def src_encode_map_fn(text_tensor):\n",
        "  # py_func doesn't set the shape of the returned tensors.\n",
        "  src_encoded_text = tf.py_function(src_encode, inp=[text_tensor], Tout=tf.int64)\n",
        "  #tf.data.Datasets need to set the shapes manually \n",
        "  src_encoded_text.set_shape([None])\n",
        "  return src_encoded_text\n",
        "\n",
        "srcs_encoded_dataset = srcs_dataset.map(src_encode_map_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV_kmDgJ-_qe"
      },
      "source": [
        "Mapping tokens of target sentences into list of integers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gukF_mszP0fK"
      },
      "source": [
        "def trg_encode(text_tensor):\n",
        "  trg_encoded_text = trg_encoder.encode(text_tensor.numpy())\n",
        "  return [trg_encoded_text]\n",
        "\n",
        "def trg_encode_map_fn(text_tensor):\n",
        "  # py_func doesn't set the shape of the returned tensors.\n",
        "  trg_encoded_text = tf.py_function(trg_encode, inp=[text_tensor], Tout=tf.int64)\n",
        "  #tf.data.Datasets need to set the shapes manually \n",
        "  trg_encoded_text.set_shape([None])\n",
        "  return trg_encoded_text\n",
        "\n",
        "trgs_encoded_dataset = trgs_dataset.map(trg_encode_map_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCJTOPBHMTy9"
      },
      "source": [
        "Checking the result of applying the mapping to the source and target sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQUeDJC74gEn"
      },
      "source": [
        "for src in srcs_encoded_dataset.take(5):\n",
        "  print(src)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im0cOgDHSIqV"
      },
      "source": [
        "for trg in trgs_encoded_dataset.take(5):\n",
        "  print(trg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvmTVHBFrovc"
      },
      "source": [
        "First, zipping source and target datasets into a dataset of (source, target) sentences. Then, zipping (source target) dataset with the label dataset, so that we have a ((source, target), label) dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2ErAFTtrUYC"
      },
      "source": [
        "dataset = tf.data.Dataset.zip((tf.data.Dataset.zip((srcs_encoded_dataset, trgs_encoded_dataset)), labs_encoded_dataset)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deiJp8uRMgrm"
      },
      "source": [
        "for sample in dataset.take(5):\n",
        "  print(sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YZToSXSm0qr"
      },
      "source": [
        "## Experimental design\n",
        "\n",
        "Use `tf.data.Dataset.take` and `tf.data.Dataset.skip` to split dataset into 50% for training, 20% for validation and 30% for test.\n",
        "\n",
        "Before being passed into the model, the datasets need to be shuffled and batched. So, first, the complete dataset is shuffled with a fixed seed so that we can repeat the same shuffle of the dataset, then the dataset is split into training, validation and test, and each of these subsets is batched.\n",
        "\n",
        "Typically, the examples inside of a batch need to be the same size and shape. But, the examples in these datasets are not all the same size — each line of text had a different number of words. So use `tf.data.Dataset.padded_batch` (instead of `batch`) to pad the examples to the same size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-rmbijQh6bf"
      },
      "source": [
        "trainsz = int(numsamples*0.5)\n",
        "valsz= int(numsamples*0.2)\n",
        "testsz= int(numsamples*0.3)\n",
        "batchsz = 100\n",
        "\n",
        "dataset = dataset.shuffle(numsamples,seed=13)\n",
        "\n",
        "train_data = dataset.take(trainsz)\n",
        "train_data = train_data.padded_batch(batchsz,padded_shapes=(([None],[None]),[]))\n",
        "\n",
        "val_data = dataset.skip(trainsz).take(valsz)\n",
        "val_data = val_data.padded_batch(batchsz,padded_shapes=(([None],[None]),[]))\n",
        "\n",
        "test_data = dataset.skip(trainsz+valsz)\n",
        "test_data = test_data.padded_batch(batchsz,padded_shapes=(([None],[None]),[]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xdz7SVwmqi1l"
      },
      "source": [
        "Now, `train_data`, `val_data` and `test_data` are not collections of (`(source, target), label`) pairs, but collections of batches. Each batch is a pair of (*set of (source, target) sentences*, *set of labels*) represented as arrays. To illustrate this idea we take one batch from the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMslWfuwoqpB"
      },
      "source": [
        "sample_bitext, sample_label = next(iter(test_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_uTSbUz752e"
      },
      "source": [
        "The sample_bitext is a tuple of (source sentences, target sentences) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yJOc8947kTf"
      },
      "source": [
        "sample_bitext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrmkXp4D8_jC"
      },
      "source": [
        "The 10th source and target sentence of the batch..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_uAc6s47j5e"
      },
      "source": [
        "sample_bitext[0][10], sample_bitext[1][10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0C_ouchQ9gFx"
      },
      "source": [
        "... and the corresponding label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHG4Zrff9kds"
      },
      "source": [
        "sample_label[10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI4I6_Sa0vWu"
      },
      "source": [
        "Since we have introduced a new token encoding (the zero used for padding), the vocabulary size has increased by one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlD1Lli91vuc"
      },
      "source": [
        "src_vocab_size += 1\n",
        "trg_vocab_size += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8SUhGFNsmRi"
      },
      "source": [
        "## Build the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJzcMswJskw4"
      },
      "source": [
        "First, we define the input layers for the source sentence as an array of integers "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meB_9aDktC1Z"
      },
      "source": [
        "src_input = tf.keras.layers.Input(shape=(None,), dtype='int32', name='src_input')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z3xBkeftYiK"
      },
      "source": [
        "The embedding layer converts integer representations to dense vector embeddings. See the [word embeddings tutorial](../text/word_embeddings.ipynb) for more details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVOoc4M0tn8g"
      },
      "source": [
        "src_embed = tf.keras.layers.Embedding(output_dim=16, input_dim=src_vocab_size)(src_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0axBU7gDuRlU"
      },
      "source": [
        "Target sentences undergo the same process as source sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGDQ7tuuu3xn"
      },
      "source": [
        "trg_input = tf.keras.layers.Input(shape=(None,), dtype='int32', name='trg_input')\n",
        "trg_embed = tf.keras.layers.Embedding(output_dim=16, input_dim=trg_vocab_size)(trg_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention layer in which query is the source sequence embeddings and value is the target sequence embeddings of the second piece of text. key is usually the same tensor as value."
      ],
      "metadata": {
        "id": "ApX7pCsRU_Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_value_attention_seq = tf.keras.layers.Attention()(\n",
        "    [src_embed, trg_embed])"
      ],
      "metadata": {
        "id": "TzIHEAdeVSVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reduce over the sequence axis to produce input into a dense feed-forward network"
      ],
      "metadata": {
        "id": "XORRzUlCVxlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dense_input = tf.keras.layers.GlobalAveragePooling1D()(\n",
        "    query_value_attention_seq)"
      ],
      "metadata": {
        "id": "Ot7Ybo15Vr9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qvl_cErPwqiN"
      },
      "source": [
        "The concatenation of source and target BLSTM is input into a dense feed-forward network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2fbQ0Q7wTLn"
      },
      "source": [
        "units=16\n",
        "dense_input = tf.keras.layers.Dense(units, activation='relu')(dense_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gQyku7Kww9r"
      },
      "source": [
        "More densely connected layers can be added"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq4Z3QLuwF90"
      },
      "source": [
        "for units in [16]:\n",
        "  dense_input = tf.keras.layers.Dense(units, activation='relu')(dense_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsuNznLnxG-N"
      },
      "source": [
        "The output layer produces a probability for all the labels. The one with the highest probability is the models prediction of an example's label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0ErESBRxNak"
      },
      "source": [
        "dense_output = tf.keras.layers.Dense(num_classes, activation='softmax')(dense_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYy2Hk-PxWte"
      },
      "source": [
        "Finally, the input and output of the model is defined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV1fjarSxb1x"
      },
      "source": [
        "model = tf.keras.models.Model(inputs=[src_input, trg_input], outputs=dense_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slqOsS7ONKlj"
      },
      "source": [
        "Summary of the model to know the number of parameters to be learnt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLg8nyZPNQTg"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLHPU8q5DLi_"
      },
      "source": [
        "Finally, compile the model. For a softmax categorization model, use `sparse_categorical_crossentropy` as the loss function. You can try other optimizers, but `adam` is very common."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkTBUVO4h6Y5"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM-HLo5NDhql"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "This model running on this data produces decent results (99% accuracy on the validation set)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLtO33tNh6V8"
      },
      "source": [
        "model.fit(train_data, epochs=10, validation_data=val_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXMdJZMLPxPf"
      },
      "source": [
        "##Evaluate the model\n",
        "\n",
        "Compute accuracy on the test set (almost 99% accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTPCYf_Jh6TH"
      },
      "source": [
        "eval_loss, eval_acc = model.evaluate(test_data)\n",
        "\n",
        "print('\\nEval loss: {:.3f}, Eval accuracy: {:.3f}'.format(eval_loss, eval_acc))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
