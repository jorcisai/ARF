{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KerasTutorial-TrainingWordEmbeddingPooling.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jorcisai/ARF/blob/master/src/KerasTutorial-TrainingWordEmbeddingPooling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWeQAo0Ec_BL"
      },
      "source": [
        "#Text classifier using own trained word embeddings and pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgZ9gjmPfSnK"
      },
      "source": [
        "## Imports\n",
        "Importing standard packages and tensorflow_datasets to ease data manipulation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baYFZMW_bJHh"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DtIDiZkhAaB"
      },
      "source": [
        "Data loading from local file system. Please use the \"traveler\" dataset available in the \"dat\" directory of the GitHub:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uS3QuPcgm92"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  datafn=fn;\n",
        "\n",
        "print(\"Data file: \",datafn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vycakl4thjN2"
      },
      "source": [
        "## Load text data from local file\n",
        "\n",
        "Parsing file line by line to extract class label, source and target sentences. All three are lists of strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8L-vKS7Bh-CJ"
      },
      "source": [
        "numsamples=0\n",
        "labs=[]\n",
        "src_sents=[]\n",
        "trg_sents=[]\n",
        "for line in open(datafn):\n",
        "  numsamples+=1\n",
        "  words = line.split(\" \")\n",
        "  labs.append(words[0])\n",
        "  pos=words.index(\"#\")\n",
        "  src_sents.append(\" \".join(words[1:pos-1]))\n",
        "  trg_sents.append(\" \".join(words[pos+1:]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing"
      ],
      "metadata": {
        "id": "oiau0t135_wG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple conversion from class text label into class integer label:"
      ],
      "metadata": {
        "id": "9IkdrzQs9Yw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labset = set()\n",
        "for lab in labs:\n",
        "  labset.add(lab)\n",
        "num_classes=len(labset)\n",
        "\n",
        "lab2id = {}\n",
        "for id,lab in enumerate(labset):\n",
        "  lab2id[lab]=id\n",
        "\n",
        "for id,lab in enumerate(labs):\n",
        "  labs[id]=lab2id[lab]"
      ],
      "metadata": {
        "id": "TLtON6_g9X_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization, conversion into sequence of integers and padding:"
      ],
      "metadata": {
        "id": "pGCQ_KhP9c2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.preprocessing as prepro\n",
        "\n",
        "def tokenize(sents):\n",
        "  tokenizer = prepro.text.Tokenizer(filters='')\n",
        "  tokenizer.fit_on_texts(sents)\n",
        "  tensors = tokenizer.texts_to_sequences(sents)\n",
        "  tensors = prepro.sequence.pad_sequences(tensors,padding='post')\n",
        "\n",
        "  return tensors, tokenizer\n",
        "\n",
        "src_tensors, src_tokenizer = tokenize(src_sents)"
      ],
      "metadata": {
        "id": "m0-61aO_8_0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtZVTyI3nQ5i"
      },
      "source": [
        "Loading class labels, source (Spanish) and target (English) sentences from lists into dataset objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_CsoAaPlz8l"
      },
      "source": [
        "lab_dataset = tf.data.Dataset.from_tensor_slices(labs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0Vu4Ge2pwZa"
      },
      "source": [
        "src_dataset = tf.data.Dataset.from_tensor_slices(src_tensors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJl1pEUPHoxe"
      },
      "source": [
        "Taking a look at the class labels and source sentences after being converted into dataset type. In this example, the source sentences are used to train the model including word embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPsynLIHICqC"
      },
      "source": [
        "for lab in lab_dataset.take(5):\n",
        "  print(lab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_eCQoF3p70Z"
      },
      "source": [
        "for src in src_dataset.take(5):\n",
        "  print(src)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print out vocabulary size:"
      ],
      "metadata": {
        "id": "xz2EAKMUBQmy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nAfYR6akwkD"
      },
      "source": [
        "print(len(src_tokenizer.word_counts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvmTVHBFrovc"
      },
      "source": [
        "Combining each source sentence with its corresponding class label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2ErAFTtrUYC"
      },
      "source": [
        "dataset = tf.data.Dataset.zip((src_dataset, lab_dataset)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deiJp8uRMgrm"
      },
      "source": [
        "for sample in dataset.take(5):\n",
        "  print(sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YZToSXSm0qr"
      },
      "source": [
        "## Experimental design\n",
        "\n",
        "Use `tf.data.Dataset.take` and `tf.data.Dataset.skip` to split dataset into 50% for training, 20% for validation and 30% for test.\n",
        "\n",
        "Before being passed into the model, the datasets need to be shuffled and batched. So, first, the complete dataset is shuffled with a fixed seed so that we can repeat the same shuffle of the dataset, then the dataset is split into training, validation and test, and each of these subsets is batched."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-rmbijQh6bf"
      },
      "source": [
        "trainsz = int(numsamples*0.5)\n",
        "valsz= int(numsamples*0.2)\n",
        "testsz= int(numsamples*0.3)\n",
        "batchsz = 100\n",
        "\n",
        "dataset = dataset.shuffle(numsamples,seed=13)\n",
        "\n",
        "train_data = dataset.take(trainsz)\n",
        "train_data = train_data.batch(batchsz)\n",
        "\n",
        "val_data = dataset.skip(trainsz).take(valsz)\n",
        "val_data = val_data.batch(batchsz)\n",
        "\n",
        "test_data = dataset.skip(trainsz+valsz)\n",
        "test_data = test_data.batch(batchsz)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xdz7SVwmqi1l"
      },
      "source": [
        "Now, `train_data`, `val_data` and `test_data` are not collections of (`sentence, label`) pairs, but collections of batches. Each batch is a pair of (*set of sentences*, *set of labels*) represented as arrays. To illustrate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMslWfuwoqpB"
      },
      "source": [
        "sample_text, sample_labels = next(iter(test_data))\n",
        "\n",
        "sample_text[0], sample_labels[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8SUhGFNsmRi"
      },
      "source": [
        "## Build the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksWiLXt0VdWk"
      },
      "source": [
        "Create an empty model and add layers to it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJgI1pow2YR9"
      },
      "source": [
        "model = tf.keras.Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi0iiKLTKdoF"
      },
      "source": [
        "The first layer converts integer representations to dense vector embeddings. See the [word embeddings tutorial](../text/word_embeddings.ipynb) or more details. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR6-ctbY638P"
      },
      "source": [
        "vcb_size=len(src_tokenizer.word_counts)\n",
        "model.add(tf.keras.layers.Embedding(vcb_size, 16))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8OJOPohKh1q"
      },
      "source": [
        "Next, a GlobalAveragePooling1D layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6rnq6DN_WUs"
      },
      "source": [
        "model.add(tf.keras.layers.GlobalAveragePooling1D())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdffbMr5LF1g"
      },
      "source": [
        "Finally we'll have a series of one or more densely connected layers, with the last one being the output layer. The output layer produces a probability for all the labels. The one with the highest probability is the models prediction of an example's label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTEaNSnLCsv5"
      },
      "source": [
        "# One or more dense layers.\n",
        "# Edit the list in the `for` line to experiment with layer sizes.\n",
        "for units in [16, 16]:\n",
        "  model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
        "\n",
        "# Output layer. The first argument is the number of labels.\n",
        "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tk0FovSG0ZV"
      },
      "source": [
        "Check the number of parameters of the model per layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c6AWwOvGxDl"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLHPU8q5DLi_"
      },
      "source": [
        "Finally, compile the model. For a softmax categorization model, use `sparse_categorical_crossentropy` as the loss function. You can try other optimizers, but `adam` is very common."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkTBUVO4h6Y5"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM-HLo5NDhql"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "This model running on this data produces decent results (~98% accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLtO33tNh6V8"
      },
      "source": [
        "history = model.fit(train_data, epochs=10, validation_data=val_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXMdJZMLPxPf"
      },
      "source": [
        "##Evaluate the model\n",
        "\n",
        "Compute accuracy on the test set (~98% accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTPCYf_Jh6TH"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_data)\n",
        "\n",
        "print('\\nTest loss: {:.3f}, Test accuracy: {:.3f}'.format(test_loss, test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7IWg9UBNUNM"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs(history, 'accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
