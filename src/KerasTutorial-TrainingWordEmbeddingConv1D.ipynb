{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KerasTutorialTraveler.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jorcisai/ARF/blob/master/src/KerasTutorial-TrainingWordEmbeddingConv1D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NWeQAo0Ec_BL"
      },
      "source": [
        "#Text classifier using own trained word embeddings and convolutional 1D layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fgZ9gjmPfSnK"
      },
      "source": [
        "## Imports\n",
        "Importing standard packages and tensorflow_datasets to ease data manipulation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "baYFZMW_bJHh",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-DtIDiZkhAaB"
      },
      "source": [
        "Data loading from local file system. Please use the \"traveler\" dataset available in the \"dat\" directory of the GitHub:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uS3QuPcgm92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  datafn=fn;\n",
        "\n",
        "print(\"Data file: \",datafn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vycakl4thjN2",
        "colab_type": "text"
      },
      "source": [
        "## Load text data from local file\n",
        "\n",
        "Parsing file line by line to extract class label, source and target sentences. All three are lists of strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8L-vKS7Bh-CJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numsamples=0\n",
        "labs=[]\n",
        "srcs=[]\n",
        "trgs=[]\n",
        "for line in open(datafn):\n",
        "  numsamples+=1\n",
        "  words = line.split(\" \")\n",
        "  labs.append(words[0])\n",
        "  pos=words.index(\"#\")\n",
        "  srcs.append(\" \".join(words[1:pos-1]))\n",
        "  trgs.append(\" \".join(words[pos+1:]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtZVTyI3nQ5i",
        "colab_type": "text"
      },
      "source": [
        "Loading class labels, source (Spanish) and target (English) sentences from lists into dataset objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_CsoAaPlz8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labs_dataset = tf.data.Dataset.from_tensor_slices(labs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0Vu4Ge2pwZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "srcs_dataset = tf.data.Dataset.from_tensor_slices(srcs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZVk8SbWVLsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trgs_dataset = tf.data.Dataset.from_tensor_slices(trgs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJl1pEUPHoxe",
        "colab_type": "text"
      },
      "source": [
        "Taking a look at the class labels and source sentences after being converted into dataset type. In this example, the source sentences are used to train the model including word embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPsynLIHICqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for lab in labs_dataset.take(5):\n",
        "  print(lab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_eCQoF3p70Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for src in srcs_dataset.take(5):\n",
        "  print(src)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsrImw-HQ5y6",
        "colab_type": "text"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHA4dJhCQ7Zk",
        "colab_type": "text"
      },
      "source": [
        "Obtaining the set of class labels to map them into integers and computing the number of classes. It requires to extract the string from the tf.Tensor object and then map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMV5TNOIQ_1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_set = set()\n",
        "for lab_tensor in labs_dataset:\n",
        "  label_set.add(lab_tensor.numpy().decode('utf-8'))\n",
        "  \n",
        "num_classes=len(label_set)\n",
        "\n",
        "lab2id = {}\n",
        "for lab_id,lab in enumerate(label_set):\n",
        "  lab2id[lab]=lab_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6vZ_v_iRD99",
        "colab_type": "text"
      },
      "source": [
        "You can check the assignment of class label to integer label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDtP18MWRHHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for key,value in lab2id.items():\n",
        "  print (key,value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEgTAK3M_JQJ",
        "colab_type": "text"
      },
      "source": [
        "Apply `Dataset.map` to each element of the dataset using the encoder as a function. `Dataset.map` runs in graph mode.\n",
        "\n",
        "* Graph tensors do not have a value. \n",
        "* In graph mode you can only use TensorFlow Ops and functions. \n",
        "\n",
        "So you can't `.map` this function directly: You need to wrap it in a `tf.py_function`. The `tf.py_function` will pass regular tensors (with a value and a `.numpy()` method to access it), to the wrapped python function.\n",
        "\n",
        "Converting author labels <b>A</b>, <b>F</b>, <b>J</b> and <b>P</b> into integers for the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IyQM-54ne08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def author_labeler(text_lab: tf.Tensor):\n",
        "  int_lab = lab2id[text_lab.numpy().decode('utf-8')]\n",
        "  return int_lab\n",
        "\n",
        "def lab_map_fn(text_lab):\n",
        "  # py_func doesn't set the shape of the returned tensors.\n",
        "  int_lab = tf.py_function(func=author_labeler, inp=[text_lab], Tout=tf.int64)\n",
        "  # tf.data.Datasets need to set the shapes manually\n",
        "  int_lab.set_shape([])\n",
        "  return int_lab\n",
        "\n",
        "labs_encoded_dataset = labs_dataset.map(lab_map_fn)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W_hx64NHWsN",
        "colab_type": "text"
      },
      "source": [
        "You can check how a few class samples look like after being mapped "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02-jGUtioX0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for lab in labs_encoded_dataset.take(5):\n",
        "  print(lab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPA5d3kcoHVr",
        "colab_type": "text"
      },
      "source": [
        "Build the vocabulary of a set of sentences by tokenizing each sentence and adding the resulting tokens into a set of of individual unique words. For this tutorial:\n",
        "<ol>\n",
        "<li> Iterate over each sentence's numpy value.</li>\n",
        "<li> Use tfds.deprecated.text.Tokenizer to split it into tokens.</li>\n",
        "<li> Collect these tokens into a Python set, to remove duplicates.</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOt34nTOAm48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = tfds.deprecated.text.Tokenizer()\n",
        "\n",
        "vocabulary_set = set()\n",
        "for text_tensor in srcs_dataset:\n",
        "  tokens = tokenizer.tokenize(text_tensor.numpy())\n",
        "  vocabulary_set.update(tokens)\n",
        "  \n",
        "vocab_size=len(vocabulary_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85b8QjXSGkV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(vocabulary_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nAfYR6akwkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylqvz4D4pNhG",
        "colab_type": "text"
      },
      "source": [
        "Create an encoder by passing the vocabulary_set to tfds.deprecated.text.TokenTextEncoder. The encoder's encode method takes in a string of text and returns a list of integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ama-wKuyBLkZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = tfds.deprecated.text.TokenTextEncoder(vocabulary_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVMcUXwcEoXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for src in srcs_dataset.take(5):\n",
        "  print(encoder.encode(src.numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kplPAlVwpc3v",
        "colab_type": "text"
      },
      "source": [
        "Mapping tokens of source sentences into list of integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTZ4qc3bCfQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode(text_tensor):\n",
        "  encoded_text = encoder.encode(text_tensor.numpy())\n",
        "  return [encoded_text]\n",
        "\n",
        "def encode_map_fn(text_tensor):\n",
        "  # py_func doesn't set the shape of the returned tensors.\n",
        "  encoded_text = tf.py_function(encode, inp=[text_tensor], Tout=tf.int64)\n",
        "  #tf.data.Datasets need to set the shapes manually \n",
        "  encoded_text.set_shape([None])\n",
        "  return encoded_text\n",
        "\n",
        "srcs_encoded_dataset = srcs_dataset.map(encode_map_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCJTOPBHMTy9",
        "colab_type": "text"
      },
      "source": [
        "Checking the result of applying the mapping to the source sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQUeDJC74gEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for src in srcs_encoded_dataset.take(5):\n",
        "  print(src)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvmTVHBFrovc",
        "colab_type": "text"
      },
      "source": [
        "Combining each source sentence with its corresponding class label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2ErAFTtrUYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.zip((srcs_encoded_dataset, labs_encoded_dataset)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deiJp8uRMgrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for sample in dataset.take(5):\n",
        "  print(sample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_YZToSXSm0qr"
      },
      "source": [
        "## Experimental design\n",
        "\n",
        "Use `tf.data.Dataset.take` and `tf.data.Dataset.skip` to split dataset into 50% for training, 20% for validation and 30% for test.\n",
        "\n",
        "Before being passed into the model, the datasets need to be shuffled and batched. So, first, the complete dataset is shuffled with a fixed seed so that we can repeat the same shuffle of the dataset, then the dataset is split into training, validation and test, and each of these subsets is batched.\n",
        "\n",
        "Typically, the examples inside of a batch need to be the same size and shape. But, the examples in these datasets are not all the same size â€” each line of text had a different number of words. So use `tf.data.Dataset.padded_batch` (instead of `batch`) to pad the examples to the same size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r-rmbijQh6bf",
        "colab": {}
      },
      "source": [
        "trainsz = int(numsamples*0.5)\n",
        "valsz= int(numsamples*0.2)\n",
        "testsz= int(numsamples*0.3)\n",
        "batchsz = 100\n",
        "\n",
        "dataset = dataset.shuffle(numsamples,seed=13)\n",
        "\n",
        "train_data = dataset.take(trainsz)\n",
        "train_data = train_data.padded_batch(batchsz,padded_shapes=([None],[]))\n",
        "\n",
        "val_data = dataset.skip(trainsz).take(valsz)\n",
        "val_data = val_data.padded_batch(batchsz,padded_shapes=([None],[]))\n",
        "\n",
        "test_data = dataset.skip(trainsz+valsz)\n",
        "test_data = test_data.padded_batch(batchsz,padded_shapes=([None],[]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xdz7SVwmqi1l"
      },
      "source": [
        "Now, `train_data`, `val_data` and `test_data` are not collections of (`sentence, label`) pairs, but collections of batches. Each batch is a pair of (*set of sentences*, *set of labels*) represented as arrays. To illustrate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kMslWfuwoqpB",
        "colab": {}
      },
      "source": [
        "sample_text, sample_labels = next(iter(test_data))\n",
        "\n",
        "sample_text[0], sample_labels[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UI4I6_Sa0vWu"
      },
      "source": [
        "Since we have introduced a new token encoding (the zero used for padding), the vocabulary size has increased by one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IlD1Lli91vuc",
        "colab": {}
      },
      "source": [
        "vocab_size += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K8SUhGFNsmRi"
      },
      "source": [
        "## Build the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksWiLXt0VdWk",
        "colab_type": "text"
      },
      "source": [
        "Create an empty model and add layers to it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QJgI1pow2YR9",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wi0iiKLTKdoF"
      },
      "source": [
        "The first layer converts integer representations to dense vector embeddings. See the [word embeddings tutorial](../text/word_embeddings.ipynb) or more details. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DR6-ctbY638P",
        "colab": {}
      },
      "source": [
        "model.add(tf.keras.layers.Embedding(vocab_size, 16))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay7FVZNdZ7gn",
        "colab_type": "text"
      },
      "source": [
        "Convolutional 1D layer with sliding window covering 5 words and output dimension of 16."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8Lv3yNrZ06_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(tf.keras.layers.Conv1D(16, 5, activation='relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_8OJOPohKh1q"
      },
      "source": [
        "Next, a GlobalAveragePooling1D layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x6rnq6DN_WUs",
        "colab": {}
      },
      "source": [
        "model.add(tf.keras.layers.GlobalAveragePooling1D())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cdffbMr5LF1g"
      },
      "source": [
        "Finally we'll have a series of one or more densely connected layers, with the last one being the output layer. The output layer produces a probability for all the labels. The one with the highest probability is the models prediction of an example's label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QTEaNSnLCsv5",
        "colab": {}
      },
      "source": [
        "# One or more dense layers.\n",
        "# Edit the list in the `for` line to experiment with layer sizes.\n",
        "for units in [16, 16]:\n",
        "  model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
        "\n",
        "# Output layer. The first argument is the number of labels.\n",
        "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tk0FovSG0ZV",
        "colab_type": "text"
      },
      "source": [
        "Check the number of parameters of the model per layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c6AWwOvGxDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zLHPU8q5DLi_"
      },
      "source": [
        "Finally, compile the model. For a softmax categorization model, use `sparse_categorical_crossentropy` as the loss function. You can try other optimizers, but `adam` is very common."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pkTBUVO4h6Y5",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DM-HLo5NDhql"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "This model running on this data produces decent results (>98% accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aLtO33tNh6V8",
        "colab": {}
      },
      "source": [
        "history = model.fit(train_data, epochs=10, validation_data=val_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXMdJZMLPxPf",
        "colab_type": "text"
      },
      "source": [
        "##Evaluate the model\n",
        "\n",
        "Compute accuracy on the test set (>98% accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KTPCYf_Jh6TH",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_data)\n",
        "\n",
        "print('\\nTest loss: {:.3f}, Test accuracy: {:.3f}'.format(test_loss, test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q7IWg9UBNUNM",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs(history, 'accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
