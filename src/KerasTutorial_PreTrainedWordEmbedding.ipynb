{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KerasTutorial-PreTrainedWordEmbedding.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWeQAo0Ec_BL"
      },
      "source": [
        "#Text classifier using pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgZ9gjmPfSnK"
      },
      "source": [
        "## Imports\n",
        "Importing standard packages and tensorflow_datasets to ease data manipulation and tensorflow_hub to provide access to pre-trained word-embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baYFZMW_bJHh"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DtIDiZkhAaB"
      },
      "source": [
        "Data loading from local file system. Please use the \"traveler\" dataset available in the \"dat\" directory of the GitHub:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uS3QuPcgm92"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  datafn=fn;\n",
        "\n",
        "print(\"Data file: \",datafn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vycakl4thjN2"
      },
      "source": [
        "## Load text data from local file\n",
        "\n",
        "Parsing file line by line to extract class label, source and target sentences. All three are lists of strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8L-vKS7Bh-CJ"
      },
      "source": [
        "numsamples=0\n",
        "labs=[]\n",
        "src_sents=[]\n",
        "trg_sents=[]\n",
        "for line in open(datafn):\n",
        "  numsamples+=1\n",
        "  words = line.split(\" \")\n",
        "  labs.append(words[0])\n",
        "  pos=words.index(\"#\")\n",
        "  src_sents.append(\" \".join(words[1:pos-1]))\n",
        "  trg_sents.append(\" \".join(words[pos+1:]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data preprocessing"
      ],
      "metadata": {
        "id": "sA9TfJ2qsCdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple conversion from class text label into class integer label: "
      ],
      "metadata": {
        "id": "oH2lfF3LyQxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labset = set()\n",
        "for lab in labs:\n",
        "  labset.add(lab)\n",
        "num_classes=len(labset)\n",
        "\n",
        "lab2id = {}\n",
        "for id,lab in enumerate(labset):\n",
        "  lab2id[lab]=id\n",
        "\n",
        "for id,lab in enumerate(labs):\n",
        "  labs[id]=lab2id[lab]"
      ],
      "metadata": {
        "id": "j0OJfgELsaWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtZVTyI3nQ5i"
      },
      "source": [
        "Loading class labels and target (English) sentences from lists into dataset objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_CsoAaPlz8l"
      },
      "source": [
        "lab_dataset = tf.data.Dataset.from_tensor_slices(labs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yLSKCB0F_Jr"
      },
      "source": [
        "trg_dataset = tf.data.Dataset.from_tensor_slices(trg_sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJl1pEUPHoxe"
      },
      "source": [
        "Taking a look at the class labels and target sentences after being converted into dataset type. In this example, as we are using a pre-trained English word embedding, only the target sentences are employed to train the text classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPsynLIHICqC"
      },
      "source": [
        "for lab in lab_dataset.take(5):\n",
        "  print(lab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for trg in trg_dataset.take(5):\n",
        "  print(trg)"
      ],
      "metadata": {
        "id": "2dE2bCZko7LW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvmTVHBFrovc"
      },
      "source": [
        "Combining each source sentence with its corresponding class label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2ErAFTtrUYC"
      },
      "source": [
        "dataset = tf.data.Dataset.zip((trg_dataset, lab_dataset)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deiJp8uRMgrm"
      },
      "source": [
        "for sample in dataset.take(5):\n",
        "  print(sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL7Fm6YqYzXE"
      },
      "source": [
        "##Pre-trained text embedding model\n",
        "\n",
        "One way to represent the text is to convert sentences into embeddings vectors. We can use a pre-trained text embedding as the first layer, which will have three advantages:\n",
        "\n",
        "*   we don't have to worry about text preprocessing,\n",
        "*   we can benefit from transfer learning,\n",
        "*   the embedding has a fixed size, so it's simpler to process.\n",
        "\n",
        "For this example we will use a **pre-trained text embedding model** from [TensorFlow Hub](https://www.tensorflow.org/hub) called [google/tf2-preview/gnews-swivel-20dim/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1).\n",
        "\n",
        "There are three other pre-trained models to test for the sake of this tutorial:\n",
        "\n",
        "* [google/tf2-preview/gnews-swivel-20dim-with-oov/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim-with-oov/1) - same as [google/tf2-preview/gnews-swivel-20dim/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1), but with 2.5% vocabulary converted to OOV buckets. This can help if vocabulary of the task and vocabulary of the model don't fully overlap.\n",
        "* [google/tf2-preview/nnlm-en-dim50/1](https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1) - A much larger model with ~1M vocabulary size and 50 dimensions.\n",
        "* [google/tf2-preview/nnlm-en-dim128/1](https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1) - Even larger model with ~1M vocabulary size and 128 dimensions.\n",
        "\n",
        "Mind that after computing the word embedding of each word of the sentence, pooling is performed to represent each sentence as a single vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVF3RXu-HhvY"
      },
      "source": [
        "embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
        "hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
        "                           dtype=tf.string, trainable=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-9_5wpXdNeO"
      },
      "source": [
        "Take a look at the resulting word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4HziuacNOS9"
      },
      "source": [
        "trg_batch,_ = next(iter(dataset.batch(5)))\n",
        "hub_layer(trg_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YZToSXSm0qr"
      },
      "source": [
        "## Experimental design\n",
        "\n",
        "Use `tf.data.Dataset.take` and `tf.data.Dataset.skip` to split dataset into 50% for training, 20% for validation and 30% for test.\n",
        "\n",
        "Before being passed into the model, the datasets need to be shuffled and batched. So, first, the complete dataset is shuffled with a fixed seed so that we can repeat the same shuffle of the dataset, then the dataset is split into training, validation and test, and each of these subsets is batched. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-rmbijQh6bf"
      },
      "source": [
        "trainsz = int(numsamples*0.5)\n",
        "valsz= int(numsamples*0.2)\n",
        "testsz= int(numsamples*0.3)\n",
        "batchsz = 100\n",
        "\n",
        "dataset = dataset.shuffle(numsamples,seed=13)\n",
        "\n",
        "train_data = dataset.take(trainsz)\n",
        "train_data = train_data.batch(batchsz)\n",
        "\n",
        "val_data = dataset.skip(trainsz).take(valsz)\n",
        "val_data = val_data.batch(batchsz)\n",
        "\n",
        "test_data = dataset.skip(trainsz+valsz)\n",
        "test_data = test_data.batch(batchsz)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xdz7SVwmqi1l"
      },
      "source": [
        "Now, `train_data`, `val_data` and `test_data` are not collections of (`sentence, label`) pairs, but collections of batches. Each batch is a pair of (*set of sentences*, *set of labels*) represented as arrays. To illustrate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMslWfuwoqpB"
      },
      "source": [
        "sample_text, sample_labels = next(iter(test_data))\n",
        "\n",
        "sample_text[0], sample_labels[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8SUhGFNsmRi"
      },
      "source": [
        "## Build the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y48CU_ZHjnJ"
      },
      "source": [
        "Create an empty model and add layers to it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJgI1pow2YR9"
      },
      "source": [
        "model = tf.keras.Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi0iiKLTKdoF"
      },
      "source": [
        "The first layer converts string representations to fixed-length word embedding using pooling. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR6-ctbY638P"
      },
      "source": [
        "model.add(hub_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdffbMr5LF1g"
      },
      "source": [
        "Finally we'll have a series of one or more densely connected layers, with the last one being the output layer. The output layer produces a probability for all the labels. The one with the highest probability is the models prediction of a sentence's label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTEaNSnLCsv5"
      },
      "source": [
        "# One or more dense layers.\n",
        "# Edit the list in the `for` line to experiment with layer sizes.\n",
        "for units in [16,16]:\n",
        "  model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
        "\n",
        "# Output layer. The first argument is the number of labels.\n",
        "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLHPU8q5DLi_"
      },
      "source": [
        "Finally, compile the model. For a softmax categorization model, use `sparse_categorical_crossentropy` as the loss function. You can try other optimizers, but `adam` is very common."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkTBUVO4h6Y5"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM-HLo5NDhql"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "This model running on this data produces decent results (>98% accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLtO33tNh6V8"
      },
      "source": [
        "model.fit(train_data, epochs=10, validation_data=val_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXMdJZMLPxPf"
      },
      "source": [
        "##Evaluate the model\n",
        "\n",
        "Compute accuracy on the test set (>97% accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTPCYf_Jh6TH"
      },
      "source": [
        "eval_loss, eval_acc = model.evaluate(test_data)\n",
        "\n",
        "print('\\nEval loss: {:.3f}, Eval accuracy: {:.3f}'.format(eval_loss, eval_acc))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}