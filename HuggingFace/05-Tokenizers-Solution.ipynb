{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlFBSYOxDvn06pVqOjZjX9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizers"
      ],
      "metadata": {
        "id": "ilDzuxzsAkOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**: Use pre-trained SentencePiece and Encoder from [T5 model](https://huggingface.co/docs/transformers/model_doc/t5) and apply them to the sample sentence ”Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer”. "
      ],
      "metadata": {
        "id": "KXn_1rehFjBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "RRBpFSbEBscn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "sentence=\"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\"\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk7s1L15FstP",
        "outputId": "48e86544-e7b6-4bd5-cc9e-5c6232775168"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁Explo', 'ring', '▁the', '▁Limit', 's', '▁of', '▁Transfer', '▁Learning', '▁with', '▁', 'a', '▁Un', 'ified', '▁Text', '-', 'to', '-', 'Tex', 't', '▁Transformer']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRgKrIX3F4Nj",
        "outputId": "5633eb24-1eaf-4870-9846-290e40ad5cc8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[19746, 1007, 8, 18185, 7, 13, 9900, 6630, 28, 3, 9, 597, 3676, 5027, 18, 235, 18, 13598, 17, 31220]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_string = tokenizer.decode(ids)\n",
        "print(decoded_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lL0zYuqAF7Xv",
        "outputId": "c88ce1ef-ef16-44bb-a702-8630a8c2fde1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n"
          ]
        }
      ]
    }
  ]
}