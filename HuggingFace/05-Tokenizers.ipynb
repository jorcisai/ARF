{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPe/PfPWbH1fij3KEZOQhH1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizers"
      ],
      "metadata": {
        "id": "ilDzuxzsAkOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In NLP, tokenizers translate text into data ready to be processed by the model. There are different approaches, such as word, character and subword level, BPE, SentencePiece, etc. Each pre-trained model is coupled with the tokenizer that was used to process its training data. As we did with models, tokenizers are loaded with the name of the checkpoint"
      ],
      "metadata": {
        "id": "sqXRjbnSAnTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "RRBpFSbEBscn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djOzyHOMAfNQ"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "More generally, we can revert to the [AutoTokenizer class](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#autotokenizer) and instanstiated by the checkpoint."
      ],
      "metadata": {
        "id": "kLjC1KEeCA06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "tokenizer(\"Using a Transformer network is simple\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCXTSPzqCFas",
        "outputId": "e09f4731-18dd-4d8c-b273-6703bbf3f48a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizers perform two processes, the actual tokenization going from text to text to reduce the vocabulary size and the encoding, converting tokens into a numerical representation. Tokenization carried out by tokenize() method of [tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer):"
      ],
      "metadata": {
        "id": "g1244Kw_Cn-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "sequence = \"Using a Transformer network is simple\"\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OYsIfI6Exut",
        "outputId": "db1e0dee-a326-44fd-c91b-20071d3a7b2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding: From tokens to input IDs handled by convert_tokens_to_ids() of [tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer):"
      ],
      "metadata": {
        "id": "PRtjj33ZE58U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7-pXGSEFIlB",
        "outputId": "205f94e7-cb8c-42d7-898c-1e99d26f7dad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoding: From vocabulary indices to string including detokenization"
      ],
      "metadata": {
        "id": "l5eIVkgXFOTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
        "print(decoded_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OT5j3qWAFRQz",
        "outputId": "3369af23-ff61-4c9b-eece-7735dbb073ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using a transformer network is simple\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**: In Colab, find pre-trained SentencePiece and Encoder models and apply them to the sample sentence ”mT5: A massively multilingual pre-trained text-to-text transformer”. [Solution](https://colab.research.google.com/github/jorcisai/ARF/blob/master/HuggingFace/05-Tokenizers-Solution.ipynb)"
      ],
      "metadata": {
        "id": "KXn_1rehFjBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code preprocess a sequence using methods of tokenizer."
      ],
      "metadata": {
        "id": "do-Rik24KToD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = torch.tensor(ids)\n",
        "print(input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z__11X-Kktm",
        "outputId": "872832cc-bcd7-4b6e-bdd8-64f3ef270a32"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
            "         2026,  2878,  2166,  1012])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the output of the previous code with the following code using tokenizer directly"
      ],
      "metadata": {
        "id": "Clse22xIKsku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
        "print(tokenized_inputs[\"input_ids\"])"
      ],
      "metadata": {
        "id": "2SRPj0cMK83-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dealing with multiple sequences"
      ],
      "metadata": {
        "id": "zzGKU1BVKMBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⊲ Multiple sentences of different length can be tokenized at the same time"
      ],
      "metadata": {
        "id": "RJoVpKXCLQDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seqs = [\"short sentence\",\"sentence with more than two words\"]\n",
        "tokenized_inputs = tokenizer(seqs)\n",
        "print(tokenized_inputs['input_ids'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3oxL0AkLU0r",
        "outputId": "3e3131cd-6dc1-4825-fbc6-c6b18da32345"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[101, 2460, 6251, 102], [101, 6251, 2007, 2062, 2084, 2048, 2616, 102]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, they cannot be converted into tensors due to different length"
      ],
      "metadata": {
        "id": "Y39LMT-KLYx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_inputs = tokenizer(seqs, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "D-xNp8tALayh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, [padding and truncation](https://huggingface.co/docs/transformers/pad_truncation) are usually necessary. First, let us see padding:"
      ],
      "metadata": {
        "id": "BWxRoZkCLmH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_inputs = tokenizer(seqs,return_tensors=\"pt\",padding=True)\n",
        "print(tokenized_inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPekZpY1LrFg",
        "outputId": "872587d1-c0b0-4350-b311-29374017bfa4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[ 101, 2460, 6251,  102,    0,    0,    0,    0],\n",
            "        [ 101, 6251, 2007, 2062, 2084, 2048, 2616,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can take a look at the first sentence after being tokenized and padded:"
      ],
      "metadata": {
        "id": "1ZHUud5gLwme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(tokenized_inputs['input_ids'][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMkx2szfL7gB",
        "outputId": "1c09bb01-e96c-4f81-82aa-4642d75fbd1d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] short sentence [SEP] [PAD] [PAD] [PAD] [PAD]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer models are trained on sequences with a maximum length and its performance degrades rapidly, if inference uses longer sequences. A straightforward solution is truncation"
      ],
      "metadata": {
        "id": "S2McOCBpMfF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_inputs = tokenizer(seqs,return_tensors=\"pt\",truncation=True,max_length=4)\n",
        "print(tokenized_inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRKic_2NM-yq",
        "outputId": "2a875a26-8900-475c-f234-c1a173ae2ff8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[ 101, 2460, 6251,  102],\n",
            "        [ 101, 6251, 2007,  102]]), 'attention_mask': tensor([[1, 1, 1, 1],\n",
            "        [1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can check out the following [notebook for further basic options on the tokenizer](https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_pt.ipynb)"
      ],
      "metadata": {
        "id": "5Gms0hYfNKwY"
      }
    }
  ]
}